{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gerekli kutuphanelerin import islemi**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Temel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Pre-processing\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from textblob import TextBlob\n",
    "\n",
    "#"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Veriyi yukleme ve bos veriden temizleme**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('src.csv')\n",
    "\n",
    "# Handle missing values\n",
    "#df = df.fillna(df.mean())\n",
    "df = df.dropna()\n",
    "\n",
    "df.to_csv('drop.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pre-procesing adimlari: Stemming ve stopwords**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct typos for each word in the comment_text column\n",
    "# df['comment_text'] = df['comment_text'].apply(lambda x: ' '.join([str(TextBlob(word).correct()) for word in x.split()]))\n",
    "# df['comment_text'] = df['comment_text'].str.replace('teh', 'the')\n",
    "\n",
    "# Perform stemming\n",
    "stemmer = PorterStemmer()\n",
    "df['comment_text'] = df['comment_text'].apply(lambda x: ' '.join([stemmer.stem(word) for word in x.split()]))\n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['comment_text'] = df['comment_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "\n",
    "df.to_csv('stem.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Toxicity degerlerinde kayip degerleri ortalama ile degistiriyoruz**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df\n",
    "# Handle missing values by replacing them with the mean value of the column\n",
    "df1 = df1.fillna(df1['severe_toxicity'].mean())\n",
    "\n",
    "# Handle typos and incorrect data types by replacing them with the correct values\n",
    "df1['severe_toxicity'] = df1['severe_toxicity'].fillna(df1['severe_toxicity'].mean())\n",
    "\n",
    "# df1.to_csv('toxicity.csv')\n",
    "# print(df1['severe_toxicity'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Birinci görev: \"is_toxic\" sutunu; Toksik değeri 0’a eşit olan yorumlar icin 0, toksik değeri 0’dan \n",
    "büyük olanlar icin 1 olacak sekilde siniflanacak şekilde ayarlanacaktır.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df2 = df1\n",
    "# Create a binary label for the severe_toxicity column\n",
    "df2['is_toxic'] = df2['severe_toxicity'].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "df2.head(25)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**İkinci görev de ise toksik değerler için bir threshold belirleyerek, üç sınıf olacak \n",
    "şekilde bir ayar yapılması beklenmektedir. Sınıflar “toksik değil”, “hafif toksik” ve \n",
    "“aşırı toksik” olacak şekilde ayarlanacaktır. Çoklu sınıflandırma yapılacaktır.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'StratifiedKFold' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/enesdiler/dev/python_projects/dataproject/toxic_detection.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/enesdiler/dev/python_projects/dataproject/toxic_detection.ipynb#X33sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m param_grid \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mclass_weight\u001b[39m\u001b[39m'\u001b[39m: [{\u001b[39m0\u001b[39m: \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m: w} \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m10\u001b[39m)]}\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/enesdiler/dev/python_projects/dataproject/toxic_detection.ipynb#X33sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Define the cross-validation splitter\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/enesdiler/dev/python_projects/dataproject/toxic_detection.ipynb#X33sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m splitter \u001b[39m=\u001b[39m StratifiedKFold(n_splits\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/enesdiler/dev/python_projects/dataproject/toxic_detection.ipynb#X33sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# Define the grid search object\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/enesdiler/dev/python_projects/dataproject/toxic_detection.ipynb#X33sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m grid_search \u001b[39m=\u001b[39m GridSearchCV(model, param_grid, cv\u001b[39m=\u001b[39msplitter, scoring\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mf1\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'StratifiedKFold' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "df3 = df2\n",
    "# Define the model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Define the parameter grid for the grid search\n",
    "param_grid = {'class_weight': [{0: 1, 1: w} for w in range(1, 10)]}\n",
    "\n",
    "# Define the cross-validation splitter\n",
    "splitter = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define the grid search object\n",
    "grid_search = GridSearchCV(model, param_grid, cv=splitter, scoring='f1')\n",
    "\n",
    "# Fit the grid search object to the data\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Print the best threshold value\n",
    "print(f'Best threshold value: {grid_search.best_params_}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\nAll the 35 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n7 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/enesdiler/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/enesdiler/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1138, in fit\n    X, y = self._validate_data(\n  File \"/Users/enesdiler/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 596, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"/Users/enesdiler/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 1074, in check_X_y\n    X = check_array(\n  File \"/Users/enesdiler/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 856, in check_array\n    array = np.asarray(array, order=order, dtype=dtype)\n  File \"/Users/enesdiler/opt/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py\", line 2064, in __array__\n    return np.asarray(self._values, dtype=dtype)\nValueError: could not convert string to float: 'realli think steven? total fantasi land ?'\n\n--------------------------------------------------------------------------------\n28 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/enesdiler/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/enesdiler/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1138, in fit\n    X, y = self._validate_data(\n  File \"/Users/enesdiler/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 596, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"/Users/enesdiler/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 1074, in check_X_y\n    X = check_array(\n  File \"/Users/enesdiler/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 856, in check_array\n    array = np.asarray(array, order=order, dtype=dtype)\n  File \"/Users/enesdiler/opt/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py\", line 2064, in __array__\n    return np.asarray(self._values, dtype=dtype)\nValueError: could not convert string to float: 'thi great story. man. wonder person yell \"shut fuck up!\" ever heard it.'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/enesdiler/dev/python_projects/dataproject/toxic_detection.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/enesdiler/dev/python_projects/dataproject/toxic_detection.ipynb#X34sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Use GridSearchCV to find the best threshold value\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/enesdiler/dev/python_projects/dataproject/toxic_detection.ipynb#X34sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m grid_search \u001b[39m=\u001b[39m GridSearchCV(model, param_grid, cv\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, scoring\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mf1\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/enesdiler/dev/python_projects/dataproject/toxic_detection.ipynb#X34sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m grid_search\u001b[39m.\u001b[39;49mfit(X, y)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/enesdiler/dev/python_projects/dataproject/toxic_detection.ipynb#X34sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m best_threshold \u001b[39m=\u001b[39m grid_search\u001b[39m.\u001b[39mbest_params_[\u001b[39m'\u001b[39m\u001b[39mclass_weight\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/enesdiler/dev/python_projects/dataproject/toxic_detection.ipynb#X34sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mBest threshold value: \u001b[39m\u001b[39m{\u001b[39;00mbest_threshold\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:875\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    869\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[1;32m    870\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    871\u001b[0m     )\n\u001b[1;32m    873\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[0;32m--> 875\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[1;32m    877\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    878\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    879\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:1375\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1374\u001b[0m     \u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1375\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:852\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    845\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m!=\u001b[39m n_candidates \u001b[39m*\u001b[39m n_splits:\n\u001b[1;32m    846\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    847\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcv.split and cv.get_n_splits returned \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    848\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39minconsistent results. Expected \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    849\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msplits, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(n_splits, \u001b[39mlen\u001b[39m(out) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m n_candidates)\n\u001b[1;32m    850\u001b[0m     )\n\u001b[0;32m--> 852\u001b[0m _warn_or_raise_about_fit_failures(out, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merror_score)\n\u001b[1;32m    854\u001b[0m \u001b[39m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[1;32m    855\u001b[0m \u001b[39m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[1;32m    856\u001b[0m \u001b[39m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[1;32m    857\u001b[0m \u001b[39m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[39mif\u001b[39;00m callable(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscoring):\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:367\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[0;34m(results, error_score)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[39mif\u001b[39;00m num_failed_fits \u001b[39m==\u001b[39m num_fits:\n\u001b[1;32m    361\u001b[0m     all_fits_failed_message \u001b[39m=\u001b[39m (\n\u001b[1;32m    362\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mAll the \u001b[39m\u001b[39m{\u001b[39;00mnum_fits\u001b[39m}\u001b[39;00m\u001b[39m fits failed.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    363\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIt is very likely that your model is misconfigured.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    364\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou can try to debug the error by setting error_score=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    365\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBelow are more details about the failures:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mfit_errors_summary\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    366\u001b[0m     )\n\u001b[0;32m--> 367\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[1;32m    369\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    370\u001b[0m     some_fits_failed_message \u001b[39m=\u001b[39m (\n\u001b[1;32m    371\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mnum_failed_fits\u001b[39m}\u001b[39;00m\u001b[39m fits failed out of a total of \u001b[39m\u001b[39m{\u001b[39;00mnum_fits\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    372\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe score on these train-test partitions for these parameters\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBelow are more details about the failures:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mfit_errors_summary\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    377\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: \nAll the 35 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n7 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/enesdiler/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/enesdiler/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1138, in fit\n    X, y = self._validate_data(\n  File \"/Users/enesdiler/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 596, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"/Users/enesdiler/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 1074, in check_X_y\n    X = check_array(\n  File \"/Users/enesdiler/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 856, in check_array\n    array = np.asarray(array, order=order, dtype=dtype)\n  File \"/Users/enesdiler/opt/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py\", line 2064, in __array__\n    return np.asarray(self._values, dtype=dtype)\nValueError: could not convert string to float: 'realli think steven? total fantasi land ?'\n\n--------------------------------------------------------------------------------\n28 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/enesdiler/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/enesdiler/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1138, in fit\n    X, y = self._validate_data(\n  File \"/Users/enesdiler/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 596, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"/Users/enesdiler/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 1074, in check_X_y\n    X = check_array(\n  File \"/Users/enesdiler/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 856, in check_array\n    array = np.asarray(array, order=order, dtype=dtype)\n  File \"/Users/enesdiler/opt/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py\", line 2064, in __array__\n    return np.asarray(self._values, dtype=dtype)\nValueError: could not convert string to float: 'thi great story. man. wonder person yell \"shut fuck up!\" ever heard it.'\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "df3 = df2\n",
    "\n",
    "# Define the model and the parameters to search over\n",
    "model = LogisticRegression()\n",
    "param_grid = {'class_weight': [{1: w} for w in [0.1, 0.2, 0.4, 0.5, 0.7, 0.9, 1]]}\n",
    "\n",
    "# Use GridSearchCV to find the best threshold value\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='f1')\n",
    "grid_search.fit(X, y)\n",
    "best_threshold = grid_search.best_params_['class_weight']\n",
    "print(f'Best threshold value: {best_threshold}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''df['class'] = df['severe_toxicity'].apply(lambda x: 0 if x == 0 else 1)\n",
    "\n",
    "X = df[['id', 'comment_text', 'severe_toxicity', 'obscene',\n",
    "        'identity_attack', 'insult', 'threat', 'asian', 'atheist',\n",
    "        'bisexual', 'black', 'buddhist', 'christian', 'female',\n",
    "        'heterosexual', 'hindu', 'homosexual_gay_or_lesbian',\n",
    "        'intellectual_or_learning_disability', 'jewish', 'latino', 'male',\n",
    "        'muslim', 'other_disability', 'other_gender',\n",
    "        'other_race_or_ethnicity', 'other_religion',\n",
    "        'other_sexual_orientation', 'physical_disability',\n",
    "        'psychiatric_or_mental_illness', 'transgender', 'white',\n",
    "        'created_date', 'publication_id', 'parent_id', 'article_id',\n",
    "        'rating', 'funny', 'wow', 'sad', 'likes', 'disagree',\n",
    "        'sexual_explicit', 'identity_annotator_count',\n",
    "        'toxicity_annotator_count']]\n",
    "y = df['class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.transform(y_test)\n",
    "\n",
    "classifier = KNeighborsClassifier()\n",
    "classifier.fit(X_train, y_train)\n",
    "# Make predictions on the testing data\n",
    "predictions = classifier.predict(X_test)\n",
    "# Evaluate the model using various performance metrics\n",
    "print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
    "print(\"F-measure:\", f1_score(y_test, predictions))\n",
    "print(\"Precision:\", precision_score(y_test, predictions))\n",
    "print(\"Recall:\", recall_score(y_test, predictions))'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# # Create a function that assigns a class based on the value of  \"severe_toxicity\"\n",
    "def get_class(x):\n",
    "    if x == 0:\n",
    "        return \"non-toxic\"\n",
    "    elif x > 0 and x <= 0.5:\n",
    "        return \"midly toxic\"\n",
    "    else:\n",
    "        return \"extremely toxic\"\n",
    "\n",
    "\n",
    "# # Create a new column called 'class' that indicates the class of each row\n",
    "df['class'] = df['severe_toxicity'].apply(get_class)\n",
    "\n",
    "# # Display the first few rows of the DataFrame to see the new 'class' column\n",
    "df.head(30)\n",
    "\n",
    "# display only \"class\" column equals to \"extremely toxic\" rows\n",
    "df[df[\"class\"] == \"extremely toxic\"]'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import preprocessing\n",
    "from sklearn import utils\n",
    "\n",
    "dff = df\n",
    "\n",
    "print(dff.head(100)[\"severe_toxicity\"])\n",
    "\n",
    "X = df[['id']]\n",
    "\n",
    "y = dff['severe_toxicity']\n",
    "\n",
    "#convert y values to categorical values\n",
    "lab = preprocessing.LabelEncoder()\n",
    "y_transformed = lab.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_transformed, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_scaled, y_train,1)\n",
    "\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "confusion_matrix(y_test, y_pred)\n",
    "\n",
    "f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(f1_score(y_test, y_pred , average='macro'))\n",
    "\n",
    "thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.9, 1.0]\n",
    "\n",
    "best_threshold = 0\n",
    "best_f1 = 0\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_threshold = [1 if x >= threshold else 0 for x in y_pred]\n",
    "    f1 = f1_score(y_test, y_pred_threshold, average='macro')\n",
    "    if f1 > best_f1:\n",
    "        best_threshold = threshold\n",
    "        best_f1 = f1\n",
    "\n",
    "print(f\"Best threshold: {best_threshold}\")\n",
    "print(f\"Best F1 score: {best_f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df, df[\"severe_toxicity\"], test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a classification model on the training data\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Use the model to predict the probabilities of severe toxicity for the test data\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Convert the predicted probabilities to binary labels using different threshold values\n",
    "thresholds = np.arange(0, 1.1, 0.1)\n",
    "y_preds = [np.array([1 if p >= t else 0 for p in y_pred_proba])\n",
    "           for t in thresholds]\n",
    "\n",
    "# Calculate the F1 score for each threshold value\n",
    "scores = [f1_score(y_test, y_pred) for y_pred in y_preds]\n",
    "\n",
    "# Find the threshold that results in the highest F1 score\n",
    "best_threshold = thresholds[np.argmax(scores)]\n",
    "print(f\"Best threshold: {best_threshold}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the true labels and predicted probabilities for your data\n",
    "y_true = [0, 1, 0, 1, 0, 1]\n",
    "y_pred_proba = [0.1, 0.9, 0.2, 0.8, 0.3, 0.7]\n",
    "\n",
    "# Convert the predicted probabilities to binary labels using different threshold values\n",
    "thresholds = np.arange(0, 1.1, 0.1)\n",
    "y_preds = [np.array([1 if p >= t else 0 for p in y_pred_proba])\n",
    "           for t in thresholds]\n",
    "\n",
    "# Calculate the F1 score for each threshold value\n",
    "scores = [f1_score(y_true, y_pred) for y_pred in y_preds]\n",
    "\n",
    "# Find the threshold that results in the highest F1 score\n",
    "best_threshold = thresholds[np.argmax(scores)]\n",
    "print(f\"Best threshold: {best_threshold}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[\"severe_toxicity\"], df[\"target\"], test_size=0.2)\n",
    "\n",
    "# Train a model on the training data\n",
    "model =  LogisticRegression() # or some other model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities on the test data\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Convert the predicted probabilities to binary labels using different threshold values\n",
    "thresholds = np.arange(0, 1.1, 0.1)\n",
    "y_preds = [np.array([1 if p >= t else 0 for p in y_pred_proba])\n",
    "           for t in thresholds]\n",
    "\n",
    "# Calculate the F1 score for each threshold value\n",
    "scores = [f1_score(y_test, y_pred) for y_pred in y_preds]\n",
    "\n",
    "# Find the threshold that results in the highest F1 score\n",
    "best_threshold = thresholds[np.argmax(scores)]\n",
    "print(f\"Best threshold: {best_threshold}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the true labels and predicted probabilities for the \"severe_toxicity\" column\n",
    "y_true = df[\"severe_toxicity\"]\n",
    "y_pred_proba = df[\"severe_toxicity_predicted_probability\"]\n",
    "\n",
    "# Convert the predicted probabilities to binary labels using different threshold values\n",
    "thresholds = np.arange(0, 1.1, 0.1)\n",
    "y_preds = [np.array([1 if p >= t else 0 for p in y_pred_proba])\n",
    "           for t in thresholds]\n",
    "\n",
    "# Calculate the F1 score for each threshold value\n",
    "scores = [f1_score(y_true, y_pred) for y_pred in y_preds]\n",
    "\n",
    "# Find the threshold that results in the highest F1 score\n",
    "best_threshold = thresholds[np.argmax(scores)]\n",
    "print(f\"Best threshold: {best_threshold}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the classification function\n",
    "def classify_toxicity(x):\n",
    "    if x < 0.5:\n",
    "        return 'non-toxic'\n",
    "    elif x < 0.75:\n",
    "        return 'mildly toxic'\n",
    "    else:\n",
    "        return 'extremely toxic'\n",
    "\n",
    "# Apply the classification function to the \"severe_toxicity\" column\n",
    "df['class'] = df['severe_toxicity'].apply(classify_toxicity)\n",
    "\n",
    "# Preview the data\n",
    "df.head()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X = df.drop(['class', 'severe_toxicity'], axis=1)\n",
    "y = df['class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = model.score(X_test, y_test)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_recall_curve, auc, roc_auc_score\n",
    "\n",
    "X = df.drop(columns=['id', 'severe_toxicity'])\n",
    "y = df['severe_toxicity']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a random forest classifier on the training set\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set and compute the precision-recall curve\n",
    "y_pred = clf.predict_proba(X_test)[:,1]\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_pred)\n",
    "\n",
    "# Plot the precision-recall curve\n",
    "plt.plot(recall, precision)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.show()\n",
    "\n",
    "# Compute the area under the curve (AUC)\n",
    "auc_score = auc(recall, precision)\n",
    "print(f'AUC: {auc_score:.2f}')\n",
    "\n",
    "# Set the threshold value to be the point on the curve with the highest precision\n",
    "threshold = thresholds[np.argmax(precision)]\n",
    "print(f'Threshold: {threshold:.2f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "89ab4a5685ff2edc1f7ef6dc68687bd8b88326787827a1c15a17a721901531f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
