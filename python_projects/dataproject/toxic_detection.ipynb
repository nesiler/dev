{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "import nltk  # Natural Language Toolkit\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id  target                                       comment_text  \\\n",
      "11   239579  0.4400  This is a great story. Man. I wonder if the pe...   \n",
      "31   239607  0.9125  Yet call out all Muslims for the acts of a few...   \n",
      "51   239644  0.0000  Because the people who drive cars more are the...   \n",
      "58   239653  0.3000  Mormons have had a complicated relationship wi...   \n",
      "111  239744  0.0000                       I'm doing the same thing! :)   \n",
      "\n",
      "     severe_toxicity   obscene  identity_attack  insult  threat  asian  \\\n",
      "11              0.00  0.293333           0.0000  0.3200  0.0000    0.0   \n",
      "31              0.05  0.237500           0.6125  0.8875  0.1125    0.0   \n",
      "51              0.00  0.000000           0.0000  0.0000  0.0000    0.0   \n",
      "58              0.00  0.000000           0.3000  0.0000  0.0000    0.0   \n",
      "111             0.00  0.000000           0.0000  0.0000  0.0000    0.0   \n",
      "\n",
      "     atheist  ...  article_id    rating  funny  wow  sad  likes  disagree  \\\n",
      "11       0.0  ...       26650  approved      0    0    0      1         0   \n",
      "31       0.0  ...       26670  approved      0    0    0      1         0   \n",
      "51       0.0  ...       26673  approved      0    0    0      0         0   \n",
      "58       0.0  ...       26670  approved      0    0    0      2         0   \n",
      "111      0.0  ...       26795  approved      0    0    0      1         0   \n",
      "\n",
      "     sexual_explicit  identity_annotator_count  toxicity_annotator_count  \n",
      "11              0.04                        10                        75  \n",
      "31              0.00                         4                        80  \n",
      "51              0.00                         4                         4  \n",
      "58              0.00                        10                        10  \n",
      "111             0.00                         4                         4  \n",
      "\n",
      "[5 rows x 45 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load the data into a pandas dataframe\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "# Handle missing values\n",
    "df = df.dropna()  # Remove rows with missing values\n",
    "\n",
    "# Perform stemming\n",
    "# nltk.download(\"porter_test\") # Download the stemmer\n",
    "# stemmer = nltk.stem.porter.PorterStemmer() # Create a stemmer object\n",
    "# df[\"comment_text\"] = df[\"comment_text\"].apply(lambda x: \" \".join([stemmer.stem(word) for word in x.split()]))\n",
    "\n",
    "# Remove stopwords\n",
    "# nltk.download(\"stopwords\") # Download the list of stopwords\n",
    "# stopwords = nltk.corpus.stopwords.words(\"english\") # Create a list of stopwords\n",
    "# df[\"comment_text\"] = df[\"comment_text\"].apply(lambda x: \" \".join([word for word in x.split() if word not in stopwords]))\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''df['class'] = df['severe_toxicity'].apply(lambda x: 0 if x == 0 else 1)\n",
    "\n",
    "X = df[['id', 'comment_text', 'severe_toxicity', 'obscene',\n",
    "        'identity_attack', 'insult', 'threat', 'asian', 'atheist',\n",
    "        'bisexual', 'black', 'buddhist', 'christian', 'female',\n",
    "        'heterosexual', 'hindu', 'homosexual_gay_or_lesbian',\n",
    "        'intellectual_or_learning_disability', 'jewish', 'latino', 'male',\n",
    "        'muslim', 'other_disability', 'other_gender',\n",
    "        'other_race_or_ethnicity', 'other_religion',\n",
    "        'other_sexual_orientation', 'physical_disability',\n",
    "        'psychiatric_or_mental_illness', 'transgender', 'white',\n",
    "        'created_date', 'publication_id', 'parent_id', 'article_id',\n",
    "        'rating', 'funny', 'wow', 'sad', 'likes', 'disagree',\n",
    "        'sexual_explicit', 'identity_annotator_count',\n",
    "        'toxicity_annotator_count']]\n",
    "y = df['class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.transform(y_test)\n",
    "\n",
    "classifier = KNeighborsClassifier()\n",
    "classifier.fit(X_train, y_train)\n",
    "# Make predictions on the testing data\n",
    "predictions = classifier.predict(X_test)\n",
    "# Evaluate the model using various performance metrics\n",
    "print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
    "print(\"F-measure:\", f1_score(y_test, predictions))\n",
    "print(\"Precision:\", precision_score(y_test, predictions))\n",
    "print(\"Recall:\", recall_score(y_test, predictions))'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Create a new column called 'class' that indicates the class of  each row\n",
    "df['class'] = df['severe_toxicity'].apply(lambda x: 0 if x == 0 else 1)\n",
    "# Display the first few rows of the DataFrame to see the new 'class' column\n",
    "df.head(25)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# # Create a function that assigns a class based on the value of  \"severe_toxicity\"\n",
    "def get_class(x):\n",
    "    if x == 0:\n",
    "        return \"non-toxic\"\n",
    "    elif x > 0 and x <= 0.5:\n",
    "        return \"midly toxic\"\n",
    "    else:\n",
    "        return \"extremely toxic\"\n",
    "\n",
    "\n",
    "# # Create a new column called 'class' that indicates the class of each row\n",
    "df['class'] = df['severe_toxicity'].apply(get_class)\n",
    "\n",
    "# # Display the first few rows of the DataFrame to see the new 'class' column\n",
    "df.head(30)\n",
    "\n",
    "# display only \"class\" column equals to \"extremely toxic\" rows\n",
    "df[df[\"class\"] == \"extremely toxic\"]'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qt/ls242cln16n150nbwhnh5g_w0000gn/T/ipykernel_23167/1440359458.py:14: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df = df.fillna(df.mean())\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Handle missing values by replacing them with the mean value of the column\n",
    "df = df.fillna(df['severe_toxicity'].mean())\n",
    "\n",
    "# # Handle typos and incorrect data types by replacing them with the correct values\n",
    "# # df['severe_toxicity'] = df['severe_toxicity'].apply(lambda x: float(x) if x.isnumeric() else np.nan)\n",
    "# df['severe_toxicity'] = df['severe_toxicity'].apply(lambda x: type(x) in [int, np.int64, float, np.float64])\n",
    "# df['severe_toxicity'] = df['severe_toxicity'].fillna(df['severe_toxicity'].mean())\n",
    "\n",
    "df = df.fillna(df.mean())\n",
    "df.to_csv('res.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords from the 'comment_text' column\n",
    "# nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['comment_text'] = df['comment_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "\n",
    "# Apply stemming to the 'comment_text' column\n",
    "stemmer = PorterStemmer()\n",
    "df['comment_text'] = df['comment_text'].apply(lambda x: ' '.join([stemmer.stem(word) for word in x.split()]))\n",
    "\n",
    "df.to_csv('res.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11      0.00\n",
      "31      0.05\n",
      "51      0.00\n",
      "58      0.00\n",
      "111     0.00\n",
      "        ... \n",
      "1548    0.00\n",
      "1557    0.00\n",
      "1566    0.00\n",
      "1578    0.00\n",
      "1586    0.00\n",
      "Name: severe_toxicity, Length: 100, dtype: float64\n",
      "0.03470089669573763\n",
      "Best threshold: 0.1\n",
      "Best F1 score: 0.03470089669573763\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import preprocessing\n",
    "from sklearn import utils\n",
    "\n",
    "dff = df\n",
    "\n",
    "print(dff.head(100)[\"severe_toxicity\"])\n",
    "\n",
    "X = df[['id']]\n",
    "\n",
    "y = dff['severe_toxicity']\n",
    "\n",
    "#convert y values to categorical values\n",
    "lab = preprocessing.LabelEncoder()\n",
    "y_transformed = lab.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_transformed, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_scaled, y_train,1)\n",
    "\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "confusion_matrix(y_test, y_pred)\n",
    "\n",
    "f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(f1_score(y_test, y_pred , average='macro'))\n",
    "\n",
    "thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.9, 1.0]\n",
    "\n",
    "best_threshold = 0\n",
    "best_f1 = 0\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_threshold = [1 if x >= threshold else 0 for x in y_pred]\n",
    "    f1 = f1_score(y_test, y_pred_threshold, average='macro')\n",
    "    if f1 > best_f1:\n",
    "        best_threshold = threshold\n",
    "        best_f1 = f1\n",
    "\n",
    "print(f\"Best threshold: {best_threshold}\")\n",
    "print(f\"Best F1 score: {best_f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df, df[\"severe_toxicity\"], test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a classification model on the training data\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Use the model to predict the probabilities of severe toxicity for the test data\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Convert the predicted probabilities to binary labels using different threshold values\n",
    "thresholds = np.arange(0, 1.1, 0.1)\n",
    "y_preds = [np.array([1 if p >= t else 0 for p in y_pred_proba])\n",
    "           for t in thresholds]\n",
    "\n",
    "# Calculate the F1 score for each threshold value\n",
    "scores = [f1_score(y_test, y_pred) for y_pred in y_preds]\n",
    "\n",
    "# Find the threshold that results in the highest F1 score\n",
    "best_threshold = thresholds[np.argmax(scores)]\n",
    "print(f\"Best threshold: {best_threshold}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the true labels and predicted probabilities for your data\n",
    "y_true = [0, 1, 0, 1, 0, 1]\n",
    "y_pred_proba = [0.1, 0.9, 0.2, 0.8, 0.3, 0.7]\n",
    "\n",
    "# Convert the predicted probabilities to binary labels using different threshold values\n",
    "thresholds = np.arange(0, 1.1, 0.1)\n",
    "y_preds = [np.array([1 if p >= t else 0 for p in y_pred_proba])\n",
    "           for t in thresholds]\n",
    "\n",
    "# Calculate the F1 score for each threshold value\n",
    "scores = [f1_score(y_true, y_pred) for y_pred in y_preds]\n",
    "\n",
    "# Find the threshold that results in the highest F1 score\n",
    "best_threshold = thresholds[np.argmax(scores)]\n",
    "print(f\"Best threshold: {best_threshold}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[\"severe_toxicity\"], df[\"target\"], test_size=0.2)\n",
    "\n",
    "# Train a model on the training data\n",
    "model =  LogisticRegression() # or some other model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities on the test data\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Convert the predicted probabilities to binary labels using different threshold values\n",
    "thresholds = np.arange(0, 1.1, 0.1)\n",
    "y_preds = [np.array([1 if p >= t else 0 for p in y_pred_proba])\n",
    "           for t in thresholds]\n",
    "\n",
    "# Calculate the F1 score for each threshold value\n",
    "scores = [f1_score(y_test, y_pred) for y_pred in y_preds]\n",
    "\n",
    "# Find the threshold that results in the highest F1 score\n",
    "best_threshold = thresholds[np.argmax(scores)]\n",
    "print(f\"Best threshold: {best_threshold}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the true labels and predicted probabilities for the \"severe_toxicity\" column\n",
    "y_true = df[\"severe_toxicity\"]\n",
    "y_pred_proba = df[\"severe_toxicity_predicted_probability\"]\n",
    "\n",
    "# Convert the predicted probabilities to binary labels using different threshold values\n",
    "thresholds = np.arange(0, 1.1, 0.1)\n",
    "y_preds = [np.array([1 if p >= t else 0 for p in y_pred_proba])\n",
    "           for t in thresholds]\n",
    "\n",
    "# Calculate the F1 score for each threshold value\n",
    "scores = [f1_score(y_true, y_pred) for y_pred in y_preds]\n",
    "\n",
    "# Find the threshold that results in the highest F1 score\n",
    "best_threshold = thresholds[np.argmax(scores)]\n",
    "print(f\"Best threshold: {best_threshold}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the classification function\n",
    "def classify_toxicity(x):\n",
    "    if x < 0.5:\n",
    "        return 'non-toxic'\n",
    "    elif x < 0.75:\n",
    "        return 'mildly toxic'\n",
    "    else:\n",
    "        return 'extremely toxic'\n",
    "\n",
    "# Apply the classification function to the \"severe_toxicity\" column\n",
    "df['class'] = df['severe_toxicity'].apply(classify_toxicity)\n",
    "\n",
    "# Preview the data\n",
    "df.head()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X = df.drop(['class', 'severe_toxicity'], axis=1)\n",
    "y = df['class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = model.score(X_test, y_test)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_recall_curve, auc, roc_auc_score\n",
    "\n",
    "X = df.drop(columns=['id', 'severe_toxicity'])\n",
    "y = df['severe_toxicity']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a random forest classifier on the training set\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set and compute the precision-recall curve\n",
    "y_pred = clf.predict_proba(X_test)[:,1]\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_pred)\n",
    "\n",
    "# Plot the precision-recall curve\n",
    "plt.plot(recall, precision)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.show()\n",
    "\n",
    "# Compute the area under the curve (AUC)\n",
    "auc_score = auc(recall, precision)\n",
    "print(f'AUC: {auc_score:.2f}')\n",
    "\n",
    "# Set the threshold value to be the point on the curve with the highest precision\n",
    "threshold = thresholds[np.argmax(precision)]\n",
    "print(f'Threshold: {threshold:.2f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 18:24:45) \n[Clang 12.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "89ab4a5685ff2edc1f7ef6dc68687bd8b88326787827a1c15a17a721901531f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
